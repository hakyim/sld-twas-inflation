---
format:
  revealjs:
    slide-number: c/t
    width: 1600
    height: 900
    css: ["theme/theme.css"]
    theme: [simple,theme/mystyle.scss]
    echo: true
editor: source
freeze: true
pdf-separate-fragments: true
---

# On the Problem of Inflation in TWAS {background-color="white"}
<br>

<h2>STATGEN 2024, Pittsburgh</h2>

<!-- <h4>Session: Identification of causal molecular phenotypes underlying complex diseases using multi-omics QTL data</h4> -->

<h3>2023-05-01 Wednesday</h3>

<h3>Hae Kyung Im</h3>


## What is TWAS?{.nostretch background-image=assets/slide-background.svg}

. . . 

<br>
It's a method that tries to identify causal genes of GWAS loci

. . . 

<br>
GWAS discoveries continue to grow but causal genes are not known


![](assets/gwas-catalog-2024-04-10.png){width=50%}

<small>downloaded April 2024 from GWAS Catalog </small>


## TWAS uses GWAS data to find genes associated with trait {background-image=assets/slide-background.svg}

![](assets/gwas-predixcan-diagram){width=1200}

::: {.absolute bottom=0 left=0 .fragment .fade-out}
![](assets/blank-rectangle.png){width="1600" height="380" fig-alt="blank"}
:::




<!-- :::{.absolute bottom=200 left=1200 .fragment .fade-in} -->
<!-- :::{.fragment .fade-out} -->

<!-- - RNA-Seq is too expensive -->
<!-- - Many tissues such has brain, pancreas, etc are not accessible -->
<!-- - Differentially expressed genes can be consequence of disease status -->

<!-- ::: -->
<!-- ::: -->

<!-- ::: {.absolute bottom=0 left=0 .fragment .fade-out} -->
<!-- ![](assets/blank-rectangle.png){width="370" height="440" fig-alt="blank"} -->
<!-- ::: -->

<!-- ::: {.absolute bottom=260 left=360 .fragment .fade-out} -->
<!-- ![](assets/blank-rectangle.png){width="110" height="30" fig-alt="blank"} -->
<!-- ::: -->

<!-- ::: {.absolute bottom=210 left=720 .fragment .fade-out} -->
<!-- ![](assets/blank-rectangle.png){width="210" height="50" fig-alt="blank"} -->
<!-- ::: -->

## Gene expression can be predicted from genotype {background-image=assets/slide-background.svg}

![](assets/reference-transcriptome-gtex.png)

## Some genes are very well predicted {background-image=assets/slide-background.svg}

![](assets/performance-top-genes.png)

## Accuracy of prediction depends on heritability {.nostretch  background-image=assets/slide-background.svg}

![](assets/performance-all-genes.png){width=75%}


## Advantages of gene level associations {background-image=assets/slide-background.svg visibility=hidden}

- Reduced multiple testing burden (from 10e6 to 10e4)
- The function of genes are much better annotated than SNPs
- Validation in other model systems is possible
- Reverse causality issues is less problematic because disease status doesn’t affect germline genetic markers 
- It provides the direction of effects, i.e. whether up or down regulation of a gene - increases the risk of a disease
- The candidate causal gene is a good target for drug development

## PrediXcan results identify drug repositioning candidates {
.nostretch 
background-image=assets/slide-background.svg 
visibility=hidden
}

![](assets/so-drug-repurposing.png){width=75%}

:::aside
So et al, Nature Neuroscience 2017
::: 

## PrediXcan Results Are Predictive of Drug Target Success {
background-image=assets/slide-background.svg
visibility=hidden}

![](assets/successful-drugs-higher-twas-signal.png)

## Limitations of TWAS  {
background-image=assets/slide-background.svg 
visibility=hidden
}

:::{.columns}

:::{.column width=75%}

- Not very good at predicting low heritability genes which are more likely to be disease relevant
- Predictions don’t transfer well across ancestries
- Effect of rare variation is missed
- Steady state eQTL may not be disease relevant

:::
:::

## TWAS extensions are actively being developed {background-image=assets/slide-background.svg .nostretch }

![](assets/omnixcan.png){width=80%}

. . . 

it uses genetic predictors of omic traits to bridge the gap between genotype and phenotype

## Leeuw et al questioned the validity of TWAS  {background-image=assets/slide-background.svg}

:::{.columns}

:::{.column width=70%}

![](assets/leeuws-twas-validity.png)

:::

:::{.column width=30%}

<br>
<br>
<br>
Leewu et al claim that noise in predictors cause inflation of TWAS

:::

:::

<!-- ## Why should we worry about inflation? {background-image=assets/slide-background.svg} -->

<!-- . . . -->

<!-- <br> -->
<!-- <br> -->

<!-- <center> -->

<!-- because none of the *WAS results would be valid -->

<!-- </center> -->

<!-- ## Hey, type I error is important but I care more about power  {background-image=assets/slide-background.svg} -->


<!-- . . . -->

<!-- <br> -->
<!-- <br> -->

<!-- <center> A reviewer actually wrote this </center> -->

<!-- . . . -->

<!-- <br>  -->
<!-- <br>  -->
<!-- <center> We will offer them a magical test with 100% power -->
<!-- <br>  -->
<!-- <br>  -->
<!-- just reject the null every time </center> -->

  

## Check inflation via simulations {background-image=assets/slide-background.svg}

:::{.columns}
  
:::{.column width=40%}
  
<br>
TWAS model
<br>

 
\begin{align*}
Y &= \beta ~ T +  \epsilon_\text{twas} \\

T &= \sum_k \gamma_k ~ X_k \quad \epsilon_\text{twas} \perp\!\!\!\perp T
\end{align*}


```{r simulation}
beta = 0
N = 1000
M = 999
maf=0.4
etwas = rnorm(N)
gamma = rnorm(M)
X = matrix(rbinom(N*M,2,maf),N,M)
T = X %*% gamma 
Y = beta * T + etwas
fit = lm(Y ~ T)
```


:::


:::{.column width=60%}

:::{.fragment}

let's simulate the regression 1000 times
<br>


```{r}
#| echo: false
nsim=1001
suppressMessages(library(tidyverse))
suppressMessages(library(devtools))
suppressMessages(source_gist("115403f16bec0a0e871f3616d552ce9b"))## load fn_ratxcan, fast regression and other convenience functions to correlate subsets of columns of two matrices

etwas = matrix(rnorm(N*nsim), N, nsim)
gamma = matrix(rnorm(M*nsim), M, nsim)
#X = matrix(rbinom(N*M,2,maf),N,M)
T = X %*% gamma 
Y = beta * T + etwas
corvec = apply( scale(Y) * scale(T), 2, sum )  / (N-1)
chi2vec = cor2chi2(corvec,N)

# hist(chi2vec,main="histogram of chi2 stat",probability = TRUE)
# 
# curve(dchisq(x, df = 1), from=0.01, add = TRUE, col = "darkgray", lwd = 3)

par(mfrow=c(1,2))
rango=range(chi2vec)
obsmeanchi2=mean(chi2vec)
qqplot(qchisq((1:nsim)/(nsim+1),1),chi2vec,
       xlab="expected", ylab="observed", xlim=rango, ylim=rango); abline(0,1); title("obs. vs. expected chi2" )
sampmeanchi2 = apply(matrix(rchisq(nsim*1000,1),nsim,1000),2,mean ) 
hist(sampmeanchi2,xlim=range(sampmeanchi2,obsmeanchi2),main="dist. of sample mean of chi2"); abline(v=mean(chi2vec),col="blue",lwd=2)
par(mfrow=c(1,1))
```


:::

::: {.fragment}
No inflation
:::


:::

:::




## What if we use a noisy predictor? {background-image=assets/slide-background.svg}

:::{.columns}
  
:::{.column width=40%}
  
<br>
TWAS model
<br>

 
\begin{align*}
Y &= \beta ~ T +  \epsilon_\text{twas} \\

\tilde{T} &= \sum_k (\gamma_k + e) ~ X_k \\

e &\perp\!\!\!\perp Y \quad \epsilon_\text{twas} \perp\!\!\!\perp T
\end{align*}


```{r noisy T simulation}
beta = 0
N = 1000
M = 999
maf=0.4
etwas = rnorm(N)
gamma = rnorm(M)
e = rnorm(M)
X = matrix(rbinom(N*M,2,maf),N,M)
Ttilde = X %*% (gamma + e)
Y = beta * Ttilde + etwas
fit = lm(Y ~ Ttilde)
```


:::


:::{.column width=60%}

:::{.fragment}

let's simulate the regression 1000 times
<br>


```{r}
#| echo: false
nsim=1001
suppressMessages(library(tidyverse))
suppressMessages(library(devtools))
suppressMessages(source_gist("115403f16bec0a0e871f3616d552ce9b"))## load fn_ratxcan, fast regression and other convenience functions to correlate subsets of columns of two matrices

etwas = matrix(rnorm(N*nsim), N, nsim)
gamma = matrix(rnorm(M*nsim), M, nsim)
e =  matrix(rnorm(M*nsim), M, nsim)
#X = matrix(rbinom(N*M,2,maf),N,M)
T = X %*% (gamma + e)
Y = beta * T + etwas
corvec = apply( scale(Y) * scale(T), 2, sum )  / (N-1)
chi2vec = cor2chi2(corvec,N)

# hist(chi2vec,main="histogram of chi2 stat",probability = TRUE)
# 
# curve(dchisq(x, df = 1), from=0.01, add = TRUE, col = "darkgray", lwd = 3)

par(mfrow=c(1,2))
rango=range(chi2vec)
obsmeanchi2=mean(chi2vec)
qqplot(qchisq((1:nsim)/(nsim+1),1),chi2vec,
       xlab="expected", ylab="observed", xlim=rango, ylim=rango); abline(0,1); title("obs. vs. expected chi2" )
sampmeanchi2 = apply(matrix(rchisq(nsim*1000,1),nsim,1000),2,mean ) 
hist(sampmeanchi2,xlim=range(sampmeanchi2,obsmeanchi2),main="dist. of sample mean of chi2"); abline(v=mean(chi2vec),col="blue",lwd=2)
par(mfrow=c(1,1))
```


:::

::: {.fragment}
No inflation
:::

::: {.fragment}
if error in T is independent of Y, 

there is no reason to see inflation 
:::

:::

:::








## Leeuw et al’s "reformulated null hypothesis" {background-image=assets/slide-background.svg}




$$  Y = \beta ~ T +  \epsilon_\text{twas} $$
$$  T = \sum_k \gamma_k ~ X_k $$



$$H_0\!:~ \beta = 0$$

. . . 

Leeuw et al claim that this is equivalent to

$$H_0\!:~ \hat\beta  = 0$$

. . .


This is not hypothesis testing is done. \{$\hat\beta=0$\} is an event with probability 0.

. . . 

Premise is wrong, and the conclusion is wrong

We can dismiss the report that error in prediction invalidates \*WAS



## The story doesn't end there, joint work with {.nostretch background-image=assets/slide-background.svg}

![](assets/yanyu-and-festus.png){width=65%}


## Let's revisit the simulation using polygenic Y {.nostretch background-image=assets/slide-background.svg}

:::{.columns}
  
:::{.column width=40%}

\begin{align*}

Y &= \beta ~ T + \epsilon_\text{twas}\\

\epsilon_\text{twas} &= \sum_k X_k ~ \delta_k + \epsilon \\

T &= \sum_k \gamma_k ~ X_k \\

\epsilon, ~\gamma,~ \delta &  \perp\!\!\!\perp ~ \therefore ~ T \perp\!\!\!\perp \epsilon_\text{twas}


\end{align*}


```{r polygenic Y simulation}
beta = 0
N = 1000
M = 999
maf=0.4
epsi = rnorm(N)
gamma = rnorm(M)
delta = rnorm(M)
X = matrix(rbinom(N*M,2,maf),N,M)
T = X %*% gamma 
Y = beta * T + X %*% delta + epsi
fit = lm(Y ~ T)
```


:::


:::{.column width=60%}

:::{.fragment}


```{r}
#| echo: false
nsim=1001
suppressMessages(library(tidyverse))
suppressMessages(library(devtools))
suppressMessages(source_gist("115403f16bec0a0e871f3616d552ce9b"))## load fn_ratxcan, fast regression and other convenience functions to correlate subsets of columns of two matrices

etwas = matrix(rnorm(N*nsim), N, nsim)
gamma = matrix(rnorm(M*nsim), M, nsim)
delta = matrix(rnorm(M*nsim), M, nsim)
#X = matrix(rbinom(N*M,2,maf),N,M)
T = X %*% gamma 
Y = beta * T + X %*% delta + etwas
corvec = apply( scale(Y) * scale(T), 2, sum )  / (N-1)
chi2vec = cor2chi2(corvec,N)

# hist(chi2vec,main="histogram of chi2 stat",probability = TRUE)
# 
# curve(dchisq(x, df = 1), from=0.01, add = TRUE, col = "darkgray", lwd = 3)

par(mfrow=c(1,2))
rango=range(chi2vec)
obsmeanchi2=mean(chi2vec)
qqplot(qchisq((1:nsim)/(nsim+1),1),chi2vec,
       xlab="expected", ylab="observed", xlim=rango, ylim=rango); abline(0,1); title("obs. vs. expected chi2" )
sampmeanchi2 = apply(matrix(rchisq(nsim*1000,1),nsim,1000),2,mean ) 
hist(sampmeanchi2,xlim=range(sampmeanchi2,obsmeanchi2),main="dist. of sample mean of chi2"); abline(v=mean(chi2vec),col="blue",lwd=2)
par(mfrow=c(1,1))
```


:::

::: {.fragment}

Now we do see inflation

:::


:::

:::

## How does inflation vary with sample size? {background-image=assets/slide-background.svg}

![](assets/twas-null-echi2-vs-nsam.png)


## How does inflation vary with heritability of Y? {background-image=assets/slide-background.svg}

![](assets/twas-null-echi2-vs-h2.png)

## How does inflation vary with the number of SNPs? {background-image=assets/slide-background.svg}

![](assets/twas-null-echi2-vs-msnp.png)


## What does the inflation look like {background-image=assets/slide-background.svg}

$$ E \chi^2 \propto N h^2 \frac{1}{M} ?$$


:::{.incremental}
- if we do the math, we get
$$ E \chi^2 \approx
1 +  N ~ h^2 ~\left ( \frac{\tilde\gamma' \cdot \Sigma^2  \cdot \tilde\gamma}{\tilde\gamma' \cdot  \Sigma  \cdot \tilde\gamma} \frac{1}{M} \right )$$


- we define the inflation factor $\Phi$ as
$$ \Phi  = \frac{\tilde\gamma' \cdot \Sigma^2  \cdot \tilde\gamma}{\tilde\gamma' \cdot  \Sigma  \cdot \tilde\gamma} \frac{1}{M} $$
- when SNPs are independent
$\Phi = \frac{1}{M}$ we get the expression we guessed
 $E \chi^2 = 1 + N h^2 \frac{1}{M}$
:::



## What are the properties of the inflation factor  {background-image=assets/slide-background.svg visibility=hidden}

:::{.incremental}

- upper and lower bounds are $$\frac{1}{M} \le \Phi \le 1$$

- lower bound attained when SNPs are independent

- upper bound attained when SNPs are perfectly correlated

- $\Phi = \frac{\text{tr}(\Sigma^2)}{M^2}$ if mediator is also polygenic

:::



## We can derive a more general expression {background-image=assets/slide-background.svg}

for the TWAS χ2 Statistic with Polygenic Y & Noisy Mediator 

<br><br>

$$E~\chi^2_\text{twas} \approx 1  + N ~h^2_\delta ~ \frac{\Phi(\text{gene})}{1 - \beta^2\tau^2} +  N ~ \frac{\beta^2 }{1 - \beta^2\tau^2}~\tau^2$$
<br><br>
<br><br>
where the precision
$\tau^2 = \text{signal to noise ratio} = \frac{\text{var}(T)}{\text{var}(\tilde{T})}$


<!-- ## Alternative χ2 with Polygenic Y and Noisy Mediator {background-image=assets/slide-background.svg} -->

<!-- ![](assets/alternative-chi2-polygenic-Y-noisy-T.png) -->

## Dependence on Precision of Predictor {background-image=assets/slide-background.svg}

![](assets/inflation-vs-precision.png)

## Can we correct the inflation? {background-image=assets/slide-background.svg}

::: {.incremental}

<br>

- Yes!
<br>

- Calculate the non centrality parameter
<br>

- Use noncentral χ2 to compute p-values
<br>

- $\texttt{pchisq(chi2, ncp=N*h2Y*phi, df=1, lower.tail=FALSE)}$

:::

## Estimated Factor Φ for Genes, Metabolites, Brain Features  {background-image=assets/slide-background.svg .nostretch}

![](assets/phi-distribution.png){width=85%}

$\Phi$ values will be included in the prediction weight databases and correction added to the PrediXcan software


## Take home message {background-image=assets/slide-background.svg}
<br>


::: {.columns}

:::{.column width=75%}
:::{.incremental}
- Standard TWAS can yield inflated p-values when the trait is highly polygenic
- Inflation can be corrected by simply using the noncentral χ2 distribution when calculating the p-values
- Error in prediction does not cause inflation (under reasonable assumptions)
- Noisy predictors can be used which will reduce false negatives, as long as the type I error is controlled
:::

:::

:::


# Thank you {background-color="white"}
